{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Single particle cropping from TIRF images**"
      ],
      "metadata": {
        "id": "S9vl0jgImcWn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pk0rfdWUhdXK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tifffile as tiff\n",
        "from numpy import savez_compressed\n",
        "\n",
        "# File paths\n",
        "csv_file_path = r'Imagej_plugin_particle_tracking_output.csv'\n",
        "tiff_path = r'TIRF_image.tif'\n",
        "output_dir = r'output_folder'\n",
        "\n",
        "# Load CSV data\n",
        "df = pd.read_csv(csv_file_path, encoding='cp949')\n",
        "df = df.iloc[3:]\n",
        "x = df.iloc[:, 3].to_numpy().astype(float)\n",
        "y = df.iloc[:, 4].to_numpy().astype(float)\n",
        "Tr = df.iloc[:, 1].to_numpy().astype(float)\n",
        "t = df.iloc[:, 2].to_numpy().astype(float)\n",
        "\n",
        "table = pd.DataFrame({'Tr': Tr, 'x': x, 'y': y, 't': t}).dropna(subset=['Tr'])\n",
        "\n",
        "# Load TIFF stack\n",
        "stack = tiff.imread(tiff_path)\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Extract and save 7x7 crops for valid trajectories\n",
        "for tr in table['Tr'].unique():\n",
        "    traj = table[table['Tr'] == tr]\n",
        "\n",
        "    if len(traj) < 10:\n",
        "        continue\n",
        "\n",
        "    if (traj['x'] < 5).any() or (traj['y'] < 5).any() or (traj['x'] > 508).any() or (traj['y'] > 508).any():\n",
        "        continue\n",
        "\n",
        "    if (traj['t'] == 0).any():  # Only save trajectories with t==0 frame\n",
        "        crop_stack = []\n",
        "        for j in range(1, len(traj)):\n",
        "            xx = int(traj.x.iloc[j])\n",
        "            yy = int(traj.y.iloc[j])\n",
        "            crop = stack[j, yy-3:yy+4, xx-3:xx+4]\n",
        "            crop_stack.append(crop)\n",
        "\n",
        "        if crop_stack:\n",
        "            output_filename = f\"Tr_{int(tr)}_x{xx}_y{yy}_t{len(traj)}.tif\"\n",
        "            output_path = os.path.join(output_dir, output_filename)\n",
        "            tiff.imwrite(output_path, crop_stack)\n",
        "\n",
        "# Search for saved .tif crop files\n",
        "file_list = [\n",
        "    fname for fname in os.listdir(output_dir)\n",
        "    if fname.endswith('.tif')\n",
        "]\n",
        "\n",
        "# Process each .tif file and save as .npz\n",
        "for file in file_list:\n",
        "    file_path = os.path.join(output_dir, file)\n",
        "    crop_stack = tiff.imread(file_path)\n",
        "\n",
        "    if crop_stack.shape[0] < 10:\n",
        "        continue\n",
        "\n",
        "    # Check if edges are non-zero (quality control)\n",
        "    if all([\n",
        "        np.any(crop_stack[:, 0, 0] != 0),\n",
        "        np.any(crop_stack[:, 6, 0] != 0),\n",
        "        np.any(crop_stack[:, 0, 6] != 0),\n",
        "        np.any(crop_stack[:, 6, 6] != 0)\n",
        "    ]):\n",
        "        # Reshape to 4D: (1, 7, 7, time)\n",
        "        crop_stack_4d = crop_stack[np.newaxis, :, :, :]\n",
        "        y_label = np.zeros((1,))  # Label = 0\n",
        "        base_name = os.path.splitext(file)[0]\n",
        "        save_path = os.path.join(output_dir, base_name + '.npz')\n",
        "        savez_compressed(save_path, crop_stack_4d, y_label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train set & test set split**"
      ],
      "metadata": {
        "id": "F-dKYv0Kn_fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Root directory containing all subfolders with npz files\n",
        "root_dir = r'Root directory'\n",
        "\n",
        "# Output directories for train and test sets\n",
        "train_dir = os.path.join(root_dir, 'training')\n",
        "test_dir = os.path.join(root_dir, 'test')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Find all folders ending with '-crop'\n",
        "crop_folders = [f.path for f in os.scandir(root_dir) if f.is_dir() and f.name.endswith('-crop')]\n",
        "print(f\"\\n📁 Found {len(crop_folders)} crop folders.\")\n",
        "\n",
        "total_files_copied = 0\n",
        "\n",
        "for crop_folder_path in crop_folders:\n",
        "    crop_folder_name = os.path.basename(crop_folder_path)\n",
        "    npz_files = [f for f in os.listdir(crop_folder_path) if f.endswith('.npz')]\n",
        "\n",
        "    if not npz_files:\n",
        "        print(f\"  ⚠️ No .npz files found in '{crop_folder_name}'\")\n",
        "        continue\n",
        "\n",
        "    # Shuffle and split the files into training and testing sets (80/20 split)\n",
        "    random.shuffle(npz_files)\n",
        "    split_index = int(len(npz_files) * 0.8)\n",
        "    train_files = npz_files[:split_index]\n",
        "    test_files = npz_files[split_index:]\n",
        "\n",
        "    # Helper function to copy files and track count\n",
        "    def copy_files(file_list, target_dir, label):\n",
        "        nonlocal total_files_copied\n",
        "        for fname in file_list:\n",
        "            src = os.path.join(crop_folder_path, fname)\n",
        "            dst_name = f\"{crop_folder_name}__{fname}\"\n",
        "            dst = os.path.join(target_dir, dst_name)\n",
        "            try:\n",
        "                shutil.copy2(src, dst)\n",
        "                total_files_copied += 1\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Error copying {fname}: {e}\")\n",
        "\n",
        "    # Copy training and test files\n",
        "    copy_files(train_files, train_dir, \"training\")\n",
        "    copy_files(test_files, test_dir, \"test\")\n",
        "\n",
        "    print(f\"  ✅ {crop_folder_name}: copied {len(train_files)} to training, {len(test_files)} to test\")\n",
        "\n",
        "# Final summary\n",
        "print(f\"\\n🎉 A total of {total_files_copied} .npz files were copied successfully.\")\n",
        "print(f\"📂 training folder: {len(os.listdir(train_dir))} files\")\n",
        "print(f\"📂 test folder: {len(os.listdir(test_dir))} files\")\n"
      ],
      "metadata": {
        "id": "OSYvsvaRn-7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data loading (classA)**"
      ],
      "metadata": {
        "id": "lg_tKCeFpE8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "#training/test folder paths\n",
        "base_path = r'root_path'\n",
        "training_path = os.path.join(base_path, 'training')\n",
        "\n",
        "def load_npz_from_folder(folder_path, use_fraction=1.0, file_fraction=1.0, folder_label=\"\"):\n",
        "    \"\"\"\n",
        "    Loads .npz files from a folder, applies sliding window, and returns selected chunks.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): Path to the folder containing .npz files.\n",
        "        use_fraction (float): Fraction of sliding windows to use per file.\n",
        "        file_fraction (float): Fraction of files to use from the folder.\n",
        "        folder_label (str): Label for logging.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Concatenated array of selected windows, or None if nothing usable.\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "    all_files = [f for f in os.listdir(folder_path) if f.endswith('.npz')]\n",
        "\n",
        "    if not all_files:\n",
        "        print(f\"⚠️ No .npz files found in '{folder_label}' folder.\")\n",
        "        return None\n",
        "\n",
        "    num_files_to_use = max(1, int(len(all_files) * file_fraction))\n",
        "    selected_files = random.sample(all_files, num_files_to_use)\n",
        "\n",
        "    print(f\"📁 {folder_label}: using {num_files_to_use} of {len(all_files)} files.\")\n",
        "\n",
        "    for filename in selected_files:\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        try:\n",
        "            arr = np.load(file_path)['arr_0']  # Expected shape: (1, 7, 7, N)\n",
        "\n",
        "            if arr.shape[0] != 1 or arr.shape[1:3] != (7, 7):\n",
        "                print(f\"  ⚠️ Skipping malformed file: {filename}\")\n",
        "                continue\n",
        "\n",
        "            arr = arr[0]  # Shape becomes: (7, 7, N)\n",
        "            N = arr.shape[2]\n",
        "\n",
        "            if N < 10:\n",
        "                print(f\"  ⚠️ Skipping short file: {filename} (N={N})\")\n",
        "                continue\n",
        "\n",
        "            # Generate sliding windows of size 10\n",
        "            num_windows = N - 10 + 1\n",
        "            windows = np.stack([arr[:, :, i:i+10] for i in range(num_windows)])\n",
        "\n",
        "            # Subsample the windows\n",
        "            select_n = max(1, int(num_windows * use_fraction))\n",
        "            selected_indices = sorted(random.sample(range(num_windows), select_n))\n",
        "            selected_windows = windows[selected_indices]\n",
        "\n",
        "            all_chunks.append(selected_windows)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Error reading '{file_path}': {e}\")\n",
        "\n",
        "    if all_chunks:\n",
        "        final_array = np.concatenate(all_chunks, axis=0)\n",
        "        print(f\"✅ {folder_label}: total loaded shape = {final_array.shape}\")\n",
        "        return final_array\n",
        "    else:\n",
        "        print(f\"⚠️ No usable data found in '{folder_label}' folder.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Define fractions to use\n",
        "train_file_fraction = 1.0  # Use 100% of training files (As long as one class is not overwhelmingly dominant)\n",
        "\n",
        "# Load training data\n",
        "print(\"\\n📂 Processing training data...\")\n",
        "classA_training_data = load_npz_from_folder(\n",
        "    training_path,\n",
        "    use_fraction=0.7, #(40%~80% of sliding windows per file, percentage varies depend on test accuracy due to overfitting issue)\n",
        "    file_fraction=train_file_fraction,\n",
        "    folder_label=\"training\"\n",
        ")\n",
        "\n",
        "# Final check\n",
        "if classA_training_data is not None:\n",
        "    print(\"✅ classA_training_data.shape =\", classA_training_data.shape)\n"
      ],
      "metadata": {
        "id": "951cYi1ZpEIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data loading (classB)**"
      ],
      "metadata": {
        "id": "Ku2tgye3rJy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "#training/test folder paths\n",
        "base_path = r'root_path'\n",
        "training_path = os.path.join(base_path, 'training')\n",
        "\n",
        "def load_npz_from_folder(folder_path, use_fraction=1.0, file_fraction=1.0, folder_label=\"\"):\n",
        "    \"\"\"\n",
        "    Loads .npz files from a folder, applies sliding window, and returns selected chunks.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): Path to the folder containing .npz files.\n",
        "        use_fraction (float): Fraction of sliding windows to use per file.\n",
        "        file_fraction (float): Fraction of files to use from the folder.\n",
        "        folder_label (str): Label for logging.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Concatenated array of selected windows, or None if nothing usable.\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "    all_files = [f for f in os.listdir(folder_path) if f.endswith('.npz')]\n",
        "\n",
        "    if not all_files:\n",
        "        print(f\"⚠️ No .npz files found in '{folder_label}' folder.\")\n",
        "        return None\n",
        "\n",
        "    num_files_to_use = max(1, int(len(all_files) * file_fraction))\n",
        "    selected_files = random.sample(all_files, num_files_to_use)\n",
        "\n",
        "    print(f\"📁 {folder_label}: using {num_files_to_use} of {len(all_files)} files.\")\n",
        "\n",
        "    for filename in selected_files:\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        try:\n",
        "            arr = np.load(file_path)['arr_0']  # Expected shape: (1, 7, 7, N)\n",
        "\n",
        "            if arr.shape[0] != 1 or arr.shape[1:3] != (7, 7):\n",
        "                print(f\"  ⚠️ Skipping malformed file: {filename}\")\n",
        "                continue\n",
        "\n",
        "            arr = arr[0]  # Shape becomes: (7, 7, N)\n",
        "            N = arr.shape[2]\n",
        "\n",
        "            if N < 10:\n",
        "                print(f\"  ⚠️ Skipping short file: {filename} (N={N})\")\n",
        "                continue\n",
        "\n",
        "            # Generate sliding windows of size 10\n",
        "            num_windows = N - 10 + 1\n",
        "            windows = np.stack([arr[:, :, i:i+10] for i in range(num_windows)])\n",
        "\n",
        "            # Subsample the windows\n",
        "            select_n = max(1, int(num_windows * use_fraction))\n",
        "            selected_indices = sorted(random.sample(range(num_windows), select_n))\n",
        "            selected_windows = windows[selected_indices]\n",
        "\n",
        "            all_chunks.append(selected_windows)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Error reading '{file_path}': {e}\")\n",
        "\n",
        "    if all_chunks:\n",
        "        final_array = np.concatenate(all_chunks, axis=0)\n",
        "        print(f\"✅ {folder_label}: total loaded shape = {final_array.shape}\")\n",
        "        return final_array\n",
        "    else:\n",
        "        print(f\"⚠️ No usable data found in '{folder_label}' folder.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Define fractions to use\n",
        "train_file_fraction = 1.0  # Use 100% of training files (As long as one class is not overwhelmingly dominant)\n",
        "\n",
        "# Load training data\n",
        "print(\"\\n📂 Processing training data...\")\n",
        "classB_training_data = load_npz_from_folder(\n",
        "    training_path,\n",
        "    use_fraction=0.7, #(40%~80% of sliding windows per file, percentage varies depend on test accuracy due to overfitting issue)\n",
        "    file_fraction=train_file_fraction,\n",
        "    folder_label=\"training\"\n",
        ")\n",
        "\n",
        "# Final check\n",
        "if classB_training_data is not None:\n",
        "    print(\"✅ classB_training_data.shape =\", classB_training_data.shape)\n"
      ],
      "metadata": {
        "id": "_Jjj-7pNqNAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data standardization**"
      ],
      "metadata": {
        "id": "rYhXEpGorMeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Combine class A and B data\n",
        "combined_X_3d = np.append(classA_training_data, classB_training_data, axis=0)\n",
        "\n",
        "# Create labels: 0 for class A, 1 for class B\n",
        "classA_labels = np.full((classA_training_data.shape[0],), 0)\n",
        "classB_labels = np.full((classB_training_data.shape[0],), 1)\n",
        "combined_y_3d = np.append(classA_labels, classB_labels, axis=0)\n",
        "\n",
        "# Copy for preprocessing\n",
        "spatial_X = np.copy(combined_X_3d)\n",
        "spatial_y = np.copy(combined_y_3d)\n",
        "\n",
        "# Normalize each sample (Z-score per 3D tensor)\n",
        "mean_vals = np.mean(spatial_X, axis=(1, 2, 3), keepdims=True)\n",
        "std_vals = np.std(spatial_X, axis=(1, 2, 3), keepdims=True)\n",
        "spatial_X = (spatial_X - mean_vals) / (std_vals + 1e-8)  # Add epsilon to avoid division by zero\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train_spa, X_test_spa, y_train, y_test = train_test_split(\n",
        "    spatial_X, spatial_y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Expand dims if needed for CNN (e.g., (N, 7, 7, 10, 1))\n",
        "if X_train_spa.ndim == 4:\n",
        "    X_train_spa = X_train_spa[..., np.newaxis]\n",
        "if X_test_spa.ndim == 4:\n",
        "    X_test_spa = X_test_spa[..., np.newaxis]"
      ],
      "metadata": {
        "id": "-0YziY8bqo3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model training**"
      ],
      "metadata": {
        "id": "ZUOd8FXHsePA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-5\n",
        "EPOCHS = 30\n",
        "\n",
        "# Save path\n",
        "model_dir = '/content/gdrive/MyDrive/gap-nick'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "model_name = f'best_model_training_bs{BATCH_SIZE}_lr{LEARNING_RATE:.0e}.keras'\n",
        "model_path = os.path.join(model_dir, model_name)\n",
        "\n",
        "# Define the model\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(7, 7, 10, 1)),\n",
        "    layers.Conv3D(32, (2, 2, 1), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.1),\n",
        "\n",
        "    layers.Conv3D(16, (3, 2, 1), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    #layers.Dropout(0.2),\n",
        "\n",
        "    layers.Conv3D(32, (2, 2, 10), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.1),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.05)),\n",
        "    layers.Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "checkpoint = ModelCheckpoint(\n",
        "    model_path,\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=6,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_spa, y_train,\n",
        "    validation_data=(X_test_spa, y_test),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[checkpoint, reduce_lr, early_stop],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)\n"
      ],
      "metadata": {
        "id": "0kJ1AELNrRMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model loading**"
      ],
      "metadata": {
        "id": "Eo9TwL_esiH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Model loading\n",
        "model = load_model('model.keras')"
      ],
      "metadata": {
        "id": "dY2jh8Eqshj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model validation**"
      ],
      "metadata": {
        "id": "htihw44ByGYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import collections\n",
        "\n",
        "# Define your trained model before this script\n",
        "# model = ...\n",
        "\n",
        "# Root directory to search for test folders\n",
        "root_dir = r'root_folder'\n",
        "\n",
        "# Find folders ending with 'test'\n",
        "crop_folders = [f.path for f in os.scandir(root_dir) if f.is_dir() and f.name.endswith('test')]\n",
        "print(f'\\n📁 Found {len(crop_folders)} test folders.\\n')\n",
        "\n",
        "summary = []\n",
        "\n",
        "for crop_dir in crop_folders:\n",
        "    folder_name = os.path.basename(crop_dir)\n",
        "    print(f\"\\n🔍 Processing folder: {folder_name}\")\n",
        "\n",
        "    file_list = [f for f in os.listdir(crop_dir) if f.endswith('.npz')]\n",
        "    if not file_list:\n",
        "        print(\"  ⚠️ No .npz files found.\")\n",
        "        continue\n",
        "\n",
        "    # Randomly select files (change ratio if needed)\n",
        "    selected_files = random.sample(file_list, int(len(file_list) * 1.0))\n",
        "\n",
        "    max_frame = 6000\n",
        "    frame = 10\n",
        "    results = np.full((max_frame, len(selected_files)), 4)  # 4 = invalid/uninitialized\n",
        "\n",
        "    for idx, filename in enumerate(selected_files):\n",
        "        file_path = os.path.join(crop_dir, filename)\n",
        "        try:\n",
        "            npz_data = np.load(file_path)\n",
        "            raw_data = npz_data['arr_0']\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Error loading {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Reshape to (7, 7, time)\n",
        "        try:\n",
        "            reshaped = np.reshape(raw_data, (7, 7, -1))\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Reshape error in {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Sliding window with shape (num_windows, 7, 7, 10)\n",
        "        try:\n",
        "            windows = np.lib.stride_tricks.sliding_window_view(reshaped, window_shape=frame, axis=2)\n",
        "            windowed_data = np.transpose(windows, axes=[2, 0, 1, 3])  # → (num_samples, 7, 7, 10)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        if windowed_data.shape[0] == 0:\n",
        "            continue\n",
        "\n",
        "        # Normalize each window individually\n",
        "        normalized = np.empty_like(windowed_data)\n",
        "        for j in range(len(windowed_data)):\n",
        "            sample = windowed_data[j]\n",
        "            mean = np.mean(sample)\n",
        "            std = np.std(sample) + 1e-8\n",
        "            normalized[j] = (sample - mean) / std\n",
        "\n",
        "        # Expand dims for 3D CNN: (batch, 7, 7, 10, 1)\n",
        "        normalized = normalized[..., np.newaxis]\n",
        "\n",
        "        try:\n",
        "            preds = model.predict(normalized, verbose=0)\n",
        "            pred_labels = np.argmax(preds, axis=1)\n",
        "            results[:len(pred_labels), idx] = pred_labels\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Prediction error on {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Post-processing predictions\n",
        "    count0, count1 = 0, 0\n",
        "    for i in range(results.shape[1]):\n",
        "        predictions = results[:, i]\n",
        "        valid_preds = predictions[predictions != 4]\n",
        "        if len(valid_preds) == 0:\n",
        "            continue\n",
        "        most_common, _ = collections.Counter(valid_preds).most_common(1)[0]\n",
        "        if most_common == 0:\n",
        "            count0 += 1\n",
        "        elif most_common == 1:\n",
        "            count1 += 1\n",
        "\n",
        "    print(f\"  ✅ Processed files: {len(selected_files)}\")\n",
        "    print(f\"     → Predicted as class 0: {count0}\")\n",
        "    print(f\"     → Predicted as class 1: {count1}\")\n",
        "\n",
        "    summary.append({\n",
        "        'folder': folder_name,\n",
        "        'total': len(selected_files),\n",
        "        'class_0': count0,\n",
        "        'class_1': count1\n",
        "    })\n",
        "\n",
        "# Final summary\n",
        "print(\"\\n📊 Summary of all predictions:\")\n",
        "for item in summary:\n",
        "    print(f\"📁 {item['folder']} — Total: {item['total']}, Class 0: {item['class_0']}, Class 1: {item['class_1']}\")\n"
      ],
      "metadata": {
        "id": "pJoYt6mYyFHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import collections\n",
        "\n",
        "# Define your trained model before this script\n",
        "# model = ...\n",
        "\n",
        "# Root directory to search for test folders\n",
        "root_dir = r'root_folder'\n",
        "\n",
        "# Find folders ending with 'test'\n",
        "crop_folders = [f.path for f in os.scandir(root_dir) if f.is_dir() and f.name.endswith('test')]\n",
        "print(f'\\n📁 Found {len(crop_folders)} test folders.\\n')\n",
        "\n",
        "summary = []\n",
        "\n",
        "for crop_dir in crop_folders:\n",
        "    folder_name = os.path.basename(crop_dir)\n",
        "    print(f\"\\n🔍 Processing folder: {folder_name}\")\n",
        "\n",
        "    file_list = [f for f in os.listdir(crop_dir) if f.endswith('.npz')]\n",
        "    if not file_list:\n",
        "        print(\"  ⚠️ No .npz files found.\")\n",
        "        continue\n",
        "\n",
        "    # Randomly select files (change ratio if needed)\n",
        "    selected_files = random.sample(file_list, int(len(file_list) * 1.0))\n",
        "\n",
        "    max_frame = 6000\n",
        "    frame = 10\n",
        "    results = np.full((max_frame, len(selected_files)), 4)  # 4 = invalid/uninitialized\n",
        "\n",
        "    for idx, filename in enumerate(selected_files):\n",
        "        file_path = os.path.join(crop_dir, filename)\n",
        "        try:\n",
        "            npz_data = np.load(file_path)\n",
        "            raw_data = npz_data['arr_0']\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Error loading {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Reshape to (7, 7, time)\n",
        "        try:\n",
        "            reshaped = np.reshape(raw_data, (7, 7, -1))\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Reshape error in {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Sliding window with shape (num_windows, 7, 7, 10)\n",
        "        try:\n",
        "            windows = np.lib.stride_tricks.sliding_window_view(reshaped, window_shape=frame, axis=2)\n",
        "            windowed_data = np.transpose(windows, axes=[2, 0, 1, 3])  # → (num_samples, 7, 7, 10)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        if windowed_data.shape[0] == 0:\n",
        "            continue\n",
        "\n",
        "        # Normalize each window individually\n",
        "        normalized = np.empty_like(windowed_data)\n",
        "        for j in range(len(windowed_data)):\n",
        "            sample = windowed_data[j]\n",
        "            mean = np.mean(sample)\n",
        "            std = np.std(sample) + 1e-8\n",
        "            normalized[j] = (sample - mean) / std\n",
        "\n",
        "        # Expand dims for 3D CNN: (batch, 7, 7, 10, 1)\n",
        "        normalized = normalized[..., np.newaxis]\n",
        "\n",
        "        try:\n",
        "            preds = model.predict(normalized, verbose=0)\n",
        "            pred_labels = np.argmax(preds, axis=1)\n",
        "            results[:len(pred_labels), idx] = pred_labels\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Prediction error on {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Post-processing predictions\n",
        "    count0, count1 = 0, 0\n",
        "    for i in range(results.shape[1]):\n",
        "        predictions = results[:, i]\n",
        "        valid_preds = predictions[predictions != 4]\n",
        "        if len(valid_preds) == 0:\n",
        "            continue\n",
        "        most_common, _ = collections.Counter(valid_preds).most_common(1)[0]\n",
        "        if most_common == 0:\n",
        "            count0 += 1\n",
        "        elif most_common == 1:\n",
        "            count1 += 1\n",
        "\n",
        "    print(f\"  ✅ Processed files: {len(selected_files)}\")\n",
        "    print(f\"     → Predicted as class 0: {count0}\")\n",
        "    print(f\"     → Predicted as class 1: {count1}\")\n",
        "\n",
        "    summary.append({\n",
        "        'folder': folder_name,\n",
        "        'total': len(selected_files),\n",
        "        'class_0': count0,\n",
        "        'class_1': count1\n",
        "    })\n",
        "\n",
        "# Final summary\n",
        "print(\"\\n📊 Summary of all predictions:\")\n",
        "for item in summary:\n",
        "    print(f\"📁 {item['folder']} — Total: {item['total']}, Class 0: {item['class_0']}, Class 1: {item['class_1']}\")\n"
      ],
      "metadata": {
        "id": "SQCLv9mc5W1W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}